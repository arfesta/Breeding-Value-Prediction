# Analysis of first two lanes of RNA-seq data - 14 May 2015
# Download to /media/seagate/LGEP, unpack tar archives, rename directories to lane01 and lane 02

# Write-protect tar archives 
chmod 444 Project_20150504-GSL028B-Whetten-X[12]POOL.tar

# Run fastqc in loop on all files in lane01 directory:
cd lane01
for file in *2.fastq.gz; do fastqc ${file}; done

# Pull out all lines with Total Sequences data from all _fastqc directories:
# Remove the lines from the html files using sed 
grep -r "Total Sequences" *_fastqc | sed '/report.html/d'
## Output:
Sample_102k_fastqc/fastqc_data.txt:Total Sequences	8220144	
Sample_105l_fastqc/fastqc_data.txt:Total Sequences	7942888	
Sample_109u_fastqc/fastqc_data.txt:Total Sequences	6712035	
Sample_10p_fastqc/fastqc_data.txt:Total Sequences	4041291	# NB - index 16 library has much lower titer
Sample_111s_fastqc/fastqc_data.txt:Total Sequences	7242237	
Sample_114r_fastqc/fastqc_data.txt:Total Sequences	8504084	
Sample_115w_fastqc/fastqc_data.txt:Total Sequences	8260834	
Sample_116x_fastqc/fastqc_data.txt:Total Sequences	8476833	
Sample_120i_fastqc/fastqc_data.txt:Total Sequences	7761997	
Sample_127h_fastqc/fastqc_data.txt:Total Sequences	8559557	
Sample_129e_fastqc/fastqc_data.txt:Total Sequences	7627322	
Sample_136c_fastqc/fastqc_data.txt:Total Sequences	7532045	
Sample_143o_fastqc/fastqc_data.txt:Total Sequences	6760539	
Sample_17a_fastqc/fastqc_data.txt:Total Sequences	8238605	
Sample_19g_fastqc/fastqc_data.txt:Total Sequences	9372423	
Sample_23n_fastqc/fastqc_data.txt:Total Sequences	7021994	
Sample_24t_fastqc/fastqc_data.txt:Total Sequences	7298847	
Sample_33j_fastqc/fastqc_data.txt:Total Sequences	8570198	
Sample_36m_fastqc/fastqc_data.txt:Total Sequences	7727714	
Sample_37d_fastqc/fastqc_data.txt:Total Sequences	8332310	
Sample_41v_fastqc/fastqc_data.txt:Total Sequences	8800520	
Sample_47q_fastqc/fastqc_data.txt:Total Sequences	7434210	
Sample_68f_fastqc/fastqc_data.txt:Total Sequences	7735911	
Sample_72b_fastqc/fastqc_data.txt:Total Sequences	8966666	


# Read counts from fastqc results for lane02 directory, using 
grep -r "Total Sequences" *_fastqc | sed '/report.html/d'
## Output
Sample_106e_fastqc/fastqc_data.txt:Total Sequences	8585008	
Sample_117v_fastqc/fastqc_data.txt:Total Sequences	6945862	
Sample_119o_fastqc/fastqc_data.txt:Total Sequences	8424483	
Sample_123d_fastqc/fastqc_data.txt:Total Sequences	8581520	
Sample_125l_fastqc/fastqc_data.txt:Total Sequences	8454370	
Sample_133a_fastqc/fastqc_data.txt:Total Sequences	7788462	
Sample_135t_fastqc/fastqc_data.txt:Total Sequences	8743581	
Sample_13f_fastqc/fastqc_data.txt:Total Sequences	8668204	
Sample_140u_fastqc/fastqc_data.txt:Total Sequences	7417893	
Sample_16w_fastqc/fastqc_data.txt:Total Sequences	7379113	
Sample_29x_fastqc/fastqc_data.txt:Total Sequences	7672881	
Sample_2q_fastqc/fastqc_data.txt:Total Sequences	8333124	
Sample_34g_fastqc/fastqc_data.txt:Total Sequences	8865123	
Sample_3s_fastqc/fastqc_data.txt:Total Sequences	7172414	
Sample_43r_fastqc/fastqc_data.txt:Total Sequences	9183316	
Sample_4j_fastqc/fastqc_data.txt:Total Sequences	8322069	
Sample_60m_fastqc/fastqc_data.txt:Total Sequences	8580571	
Sample_61i_fastqc/fastqc_data.txt:Total Sequences	9562447	
Sample_64p_fastqc/fastqc_data.txt:Total Sequences	9258861	
Sample_65n_fastqc/fastqc_data.txt:Total Sequences	7693539	
Sample_67c_fastqc/fastqc_data.txt:Total Sequences	8223051	
Sample_6b_fastqc/fastqc_data.txt:Total Sequences	8269085	
Sample_75h_fastqc/fastqc_data.txt:Total Sequences	9169233	
Sample_79k_fastqc/fastqc_data.txt:Total Sequences	8294324	

# Found bbmap suite of programs with bbduk program that can left-clip first 10 bases, filter adapters, and then quality trim
# in one step, much more quickly than any combination of fastx_toolkit, seqtk, scythe, or sickle.
# see http://seqanswers.com/forums/archive/index.php/t-42776.html for introductory post

# Create filttrim_rep1 directory for output of clipped, filtered, trimmed read files, in /media/seagate/LGEP

mkdir filttrim_rep1

# Create files with sample number+index letter from listing of lane01 and lane02 directories; use sed
# to remove the "lane0x/Sample_" prefix and ".fastq.gz" suffix from each sample ID:
ls lane01/*.gz |  sed 's%lane01/Sample_%%' | sed 's/.fastq.gz//' > lane1.ids
ls lane02/*.gz |  sed 's%lane02/Sample_%%' | sed 's/.fastq.gz//' > lane2.ids


# Use those files of ids to execute bbduk pipeline to left-clip first ten bases, filter adapters, and quality-trim to Phred20 in a bash loop:
time while read id; do ~/software/bbmap/bbduk.sh -Xmx10g in=lane01/Sample_${id}.fastq.gz out=filttrim_rep1/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < lane1.ids
# Memory use was < 1 G for each file. The time per file was typically < 1 min; the time command returned
real	20m43.612s  # for entire loop of 24 files.
user	92m6.460s
sys	2m47.115s
# Fraction of reads left ranges from 94.5% to over 98%; file sizes from 313M to 682M in filttrim_rep1 directory. Repeat for lane2:
time while read id; do ~/software/bbmap/bbduk.sh -Xmx10g in=lane02/Sample_${id}.fastq.gz out=filttrim_rep1/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < lane2.ids

# Build 
sh index of Pita.86ktxptome.fa file in /media/sg3/ptaeda directory, using k-mer of 25:
sailfish index -t Pita.86ktxptome.fa -o Pita.86xtxptome.sfidx -k 25 -p 20

# Test sailfish quant as loop to see how much RAM it requires for files of ~7 to 9 million reads,
# saving output in new directory countfiles:
mkdir countfiles
time while read id; do sailfish quant -i /media/sg3/ptaeda/Pita.86ktxptome.sfidx -l "T=SE:S=S" -r <(zcat filttrim_rep1/${id}.fq.gz) -o countfiles/${id}_rep1 -p 6 -a; done < lane1.ids

# Monitor resource usage with top: gzip uses < 50% of a CPU; sailfish uses <650% and < 1% of RAM. Should be
# OK to run three loops in parallel, one per rep, as long as nobody else is using system resources.

# Plan for next 14 lanes: rest of rep1 (lanes 3-6), all of rep2 (lanes 7-12), 4 lanes of rep3 (lanes 13-16).
# Unpack each tar archive into a lane-specific directory
# Run the bbduk.sh loop for all files in each directory - may be able to use process substitution to avoid
# creating separate lists of sample ids for each lane; e.g. for lane01, use
while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane01/Sample_${id}.fastq.gz out=filttrim_rep1/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done <(ls lane01/*.gz |  sed 's%lane01/Sample_%%' | sed 's/.fastq.gz//' )
#### *** NB *** change lane01 (3x) and filttrim_rep1 (1x) as needed for each lane and rep. ***

# With only ~ 1G of memory required per loop, all 14 can be run at the same time if t=1 option is used to limit
# to one thread per process. Test process substitution and time with one processor, saving to tmp directory:
mkdir tmp.out
time while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane01/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane01/*.gz |  sed 's%lane01/Sample_%%' | sed 's/.fastq.gz//' )

# Takes slightly longer per file using only 1 thread (79 sec rather than 64 for 102k), but advantage of doing 14 lanes
# in parallel should outweigh that slight slowdown relative to processing in series with 24 cores/job. Memory
# use with single thread is < 200M, so allocation can be reduced more if desired.
# real	27m34.782s total time used to process lane01 with 1 core.
# user	58m24.728s
# sys	1m24.604s

# Example commands for lanes 3 - 6 (rest of rep1; saved to filttrim_rep1 output directory):
while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane03/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane03/*.gz |  sed 's%lane03/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane04/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane04/*.gz |  sed 's%lane04/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane05/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane05/*.gz |  sed 's%lane05/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane06/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane06/*.gz |  sed 's%lane06/Sample_%%' | sed 's/.fastq.gz//' ) &

# After all lanes are filtered and trimmed, run sailfish against k-mer hash of Pita.86ktxptome.fa for each sample,
# using similar loop for each rep.
while read id; do sailfish quant -i /media/sg3/ptaeda/Pita.86k.sfidx -l "T=SE:S=S" -r <(zcat 115stk.fq.gz)  -o sf115stk

#### Sat May 23 2015
# List of new files on brcwebportal server, Sat May 23 2015:
[ ]	Project_20150511-GSL029-B-Whetten-Y5POOL.tar	2015-05-22 14:59 	8.1G	 
[ ]	Project_20150511-GSL029-B-Whetten-Y6POOL.tar	2015-05-22 15:00 	10G	 
[ ]	Project_20150511-GSL029-B-Whetten-Z1POOL.tar	2015-05-22 15:06 	10G	 
[ ]	Project_20150511-GSL029-B-Whetten-Z2POOL.tar	2015-05-22 15:02 	10G	 
[ ]	Project_20150511-GSL029-B-Whetten-Z3POOL.tar	2015-05-22 15:04 	9.2G	 
[ ]	Project_20150511-GSL029-B-Whetten-Z4POOL.tar	2015-05-22 15:04 	11G	 
[ ]	Project_20150511-GSL029A-Whetten-X3POOL.tar	2015-05-21 15:24 	21G	 
[ ]	Project_20150511-GSL029A-Whetten-X4POOL.tar	2015-05-21 15:25 	16G	 
[ ]	Project_20150511-GSL029A-Whetten-X5POOL.tar	2015-05-21 15:29 	16G	 
[ ]	Project_20150511-GSL029A-Whetten-X6POOL.tar	2015-05-21 15:29 	19G	 
[ ]	Project_20150511-GSL029A-Whetten-Y1POOL.tar	2015-05-21 15:31 	18G	 
[ ]	Project_20150511-GSL029A-Whetten-Y2POOL.tar	2015-05-21 15:33 	20G	 
[ ]	Project_20150511-GSL029A-Whetten-Y3POOL.tar	2015-05-21 15:33 	20G	 
[ ]	Project_20150511-GSL029A-Whetten-Y4POOL.tar	2015-05-21 15:33 	19G	 

# Run two loops in parallel to download and save files:
for id in Y5 Y6 Z1 Z2 Z3 Z4; do wget --user=whetten --password=eRaugB8z -O ${id}.tar https://brcwebportal.cos.ncsu.edu/gsl/Whetten/Project_20150511-GSL029-B-Whetten-${id}POOL.tar; done

for ID in X3 X4 X5 X6 Y1 Y2 Y3 Y4; do wget --user=whetten --password=eRaugB8z -O ${ID}.tar https://brcwebportal.cos.ncsu.edu/gsl/Whetten/Project_20150511-GSL029A-Whetten-${ID}POOL.tar; done

# It took about 5 hours to download all 14 files. Untar with a similar loop:
for file in X3 X4 X5 X6 Y1 Y2 Y3 Y4 Y5 Y6 Z1 Z2 Z3 Z4; do tar -xf ${file}.tar; done

# As each tar archive is unpacked, move the resulting directory to a more conveniently-named version:
mv Project_20150511-GSL029A-Whetten-X3POOL lane03
mv Project_20150511-GSL029A-Whetten-X4POOL lane04
mv Project_20150511-GSL029A-Whetten-X5POOL lane05
mv Project_20150511-GSL029A-Whetten-X6POOL lane06

# Run loop of bbduk commands on those four files to complete rep1; save output to filttrim_rep1 directory,
# using 
while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane03/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane03/*.gz |  sed 's%lane03/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane04/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane04/*.gz |  sed 's%lane04/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane05/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane05/*.gz |  sed 's%lane05/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do ~/software/bbmap/bbduk.sh -Xmx2g in=lane06/Sample_${id}.fastq.gz out=tmp.out/${id}.fq.gz ref=~/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane06/*.gz |  sed 's%lane06/Sample_%%' | sed 's/.fastq.gz//' ) &


#Created directory for trimmed lanes of rep2 and rep 3
mkdir filttrim_rep2
mkdir filttrim_rep3

#moved unpacked tar files to more convienent naming scheme for rep2
mv Project_20150511-GSL029A-Whetten-Y1POOL lane01_2; 
mv Project_20150511-GSL029A-Whetten-Y2POOL lane02_2;
mv Project_20150511-GSL029A-Whetten-Y3POOL lane03_2;
mv Project_20150511-GSL029A-Whetten-Y4POOL lane04_2;
mv Project_20150511-GSL029-B-Whetten-Y5POOL lane05_2;
mv Project_20150511-GSL029-B-Whetten-Y6POOL lane06_2

#Run loop of bbduk commands on all files to complete rep2; save output to filttrim_rep2 directory,
# using t=1 as specification for 1GB memory
#### *** NB *** change lane0* (3x) and filttrim_rep* (1x) as needed for each lane and rep. ***
while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane01_2/Sample_${id}.fastq.gz out=filttrim_rep2/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane01_2/*.gz |  sed 's%lane01_2/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane02_3_2/Sample_${id}.fastq.gz out=../filttrim_rep3/${id}2.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane02_3_2/*.gz |  sed 's%lane02_3_2/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane03_2/Sample_${id}.fastq.gz out=filttrim_rep2/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane03_2/*.gz |  sed 's%lane03_2/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane04_2/Sample_${id}.fastq.gz out=filttrim_rep2/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane04_2/*.gz |  sed 's%lane04_2/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane05_2/Sample_${id}.fastq.gz out=filttrim_rep2/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane05_2/*.gz |  sed 's%lane05_2/Sample_%%' | sed 's/.fastq.gz//' ) &

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane06_2/Sample_${id}.fastq.gz out=filttrim_rep2/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane06_2/*.gz |  sed 's%lane06_2/Sample_%%' | sed 's/.fastq.gz//' )

#move tar files to more convienient naming scheme for rep3 (only 4 lanes at this time)
mv Project_20150511-GSL029-B-Whetten-Z1POOL lane01_3;
mv Project_20150511-GSL029-B-Whetten-Z2POOL lane02_3;
mv Project_20150511-GSL029-B-Whetten-Z3POOL lane03_3;
mv Project_20150511-GSL029-B-Whetten-Z4POOL lane04_3;

#Run loop of bbduk commands on first 4 lanes to begin rep3; save output to filttrim_rep3 directory,
# using t=1 as specification for 1GB memory
#### *** NB *** change lane0* (3x) and filttrim_rep* (1x) as needed for each lane and rep. ***

time while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane01_3/Sample_${id}.fastq.gz out=filttrim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane01_3/*.gz |  sed 's%lane01_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane02_3/Sample_${id}.fastq.gz out=filttrim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane02_3/*.gz |  sed 's%lane02_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane03_3/Sample_${id}.fastq.gz out=filttrim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane03_3/*.gz |  sed 's%lane03_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane04_3/Sample_${id}.fastq.gz out=filttrim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane04_3/*.gz |  sed 's%lane04_3/Sample_%%' | sed 's/.fastq.gz//' )


##########################################################
#####5/29/15  Last two lanes of data from GSL have arrived
##########################################################
#### tar files were unzipped and then placed in the corresponding lane05_3 and lane06_3 directories
#### Run loop of bbduk commands on last 2 lanes of rep3 to filter and trim. Save output to filttrim_rep3 directory

time while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane05_3/Sample_${id}.fastq.gz out=filttrim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane05_3/*.gz |  sed 's%lane05_3/Sample_%%' | sed 's/.fastq.gz//' )

time while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane06_3/Sample_${id}.fastq.gz out=filttrim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane06_3/*.gz |  sed 's%lane06_3/Sample_%%' | sed 's/.fastq.gz//' )


#############Re-run Sailfish on pool3:

# Create list of sample names from rep3 files:
ls lane01_3/Sample_*.gz | sed 's%lane01_3/Sample_%%' | sed 's/.fastq.gz//' > pool3.list
ls lane02_3/Sample_*.gz | sed 's%lane02_3/Sample_%%' | sed 's/.fastq.gz//' >> pool3.list
ls lane03_3/Sample_*.gz | sed 's%lane03_3/Sample_%%' | sed 's/.fastq.gz//' >> pool3.list
ls lane04_3/Sample_*.gz | sed 's%lane04_3/Sample_%%' | sed 's/.fastq.gz//' >> pool3.list
ls lane05_3/Sample_*.gz | sed 's%lane05_3/Sample_%%' | sed 's/.fastq.gz//' >> pool3.list
ls lane06_3/Sample_*.gz | sed 's%lane06_3/Sample_%%' | sed 's/.fastq.gz//' >> pool3.list

#####put lane05_3 and lane06_3 sample names into a list
ls lane05_3/Sample_*.gz | sed 's%lane05_3/Sample_%%' | sed 's/.fastq.gz//' >> lanestorunsailfish.list
ls *2.fq.gz | sed 's/.fq.gz//' >> lanestorunsailfish.list

 
# In a different terminal window, set LD_LIBRARY_PATH variable and run Sailfish on rep3 samples that are in the filttrim_rep3
# # folder and match the lanestorunsailfish.list
###This was done because this data arrived 5/28/15 and previous sailfish run had been done on the rest of filttrim_rep1 rep2 
# # and all of rep3 except the last two lanes above:
export LD_LIBRARY_PATH=/home/ross/software/sailfish/lib
while read id; do sailfish quant -i /media/sg3/ptaeda/Pita.86ktxptome.sfidx -l "T=SE:S=S" -r <(zcat filttrim_rep3/${id}.fq.gz) -o countfiles/${id}_rep3 -p 6 -a; done < lanestorunsailfish.list

#####################Tried the below, but opted for using "Read in count files.R" ###############################
# To assemble a command line for pasting contig IDs (column1) and EstimatedNumReads (column7) from each of 384
# Sailfish output files into a single file, start with pool*.list files of sample IDs and add <(cut -f1,7 
# to beginning of line followed by space before sample ID, and _rep*/quant_bias_corrected.df ) \ after sample ID,
# with no space between \ and end of line. The double \\ at the end of the line prevents the \ from being
# understood as an escape character hiding the # sign - without the \\, the command gives a sed error about
# "unterminated 's' command". The # sign is used as a delimiter for the s command so the / in the replacement string
# is not confused with a / used as a delimiter. 
sed 's#^#<(tail -n +6 #' pool1.list | sed 's#$#_rep1/quant_bias_corrected.sf | cut -f1,7) \\#' > pool1.paste
sed 's#^#<(tail -n +6 #' pool2.list | sed 's#$#_rep2/quant_bias_corrected.sf | cut -f1,7) \\#' > pool2.paste
sed 's#^#<(tail -n +6 #' pool3.list | sed 's#$#_rep3/quant_bias_corrected.sf | cut -f1,7) \\#' > pool3.paste
cat pool1.paste pool2.paste pool3.paste > pool.all

#  use text editor to add 'paste' to beginning of first line and > 384samples.result to end of last.
### Execute the command from within the countfiles directory, so the individual sample 
### directories are visible to the cut commands. The columns will be unlabeled, but in the order in which
### the sample files are incorporated into the paste command, so merging the three pool*.list files will
### provide a list of column headings for use in R.

sed 's#^#<(tail -n +6 #' pool1.list | sed 's#$#_rep1/quant_bias_corrected.sf | cut -f1,7) \\#' > pool1.paste
sed 's#^#<(tail -n +6 #' pool2.list | sed 's#$#_rep2/quant_bias_corrected.sf | cut -f1,7) \\#' > pool2.paste
sed 's#^#<(tail -n +6 #' pool3.list | sed 's#$#_rep3/quant_bias_corrected.sf | cut -f1,7) \\#' > pool3.paste
cat pool1.paste pool2.paste pool3.paste > pool.all
##################################################################################################

# After using readincountfiles.R, analysis using GLM package was started see notes practice_GLM_analysis.R
# Take home message: GLM analysis did OK, but edgeRbatcheffect command ended up working much more efficiently to distinguish the noise due to lane and rep#



##########################################################################################
11/20/15 
#Goal: Merge fastq files of 3 techincal reps from 1 of the 47 families
# The output file is a merged fastq file that contains the three techincal reps for 1 biological replicate
while read file; do cat filt.trim_rep1/$file filt.trim_rep2/$file filt.trim_rep3/$file > ./$file ; done <samples.list


bwa075 mem -R "@RG\tID:1o\tSM:fam01" Pita.86ktxptome.fa 1o.fq.gz > 1o.sam

BWA -R "@RG\tID:1o\tSM:fam01"
bwa075 mem -R "@RG\tID:1o\tSM:fam01" Pita.86ktxptome.fa 1o.fq.gz > 4j.sam

count=1; for file in *.fq.gz; do bwa075 mem -t 12 -R "@RG\tID:${file}\tSM:fam${count}" Pita.86ktxptome.fa ${file} | samtools view -buF2308 -q20 - | samtools sort -l 5 -@ 5 -m 4G -T ./${file} -o ./bam/${file}.bam ; samtools index ./bam/${file}.bam; echo -e "@RG\tID:${file}\tSM:fam${count}" >> target.header ; ((count++)); done 

*for file in /bam/*.bam; do freebayes -v $file.vcf -f Pita.86ktxptome.fa -b $file; done

*freebayes -L samples.list -v ./allsamples.vcf -f ../Pita.86ktxptome.fa -K

samtools mpileup -b samples.list -f ../Pita.86ktxptome.fa -q 30 -v -o ./allsamp.mpileup.vcf.gz -t DP,DV
vcftools --gzvcf allsamp.mpileup.vcf.gz --out samqfilt --remove-indels --remove-filtered-all --max-maf .2 --recode
vcftools --gzvcf allsamp.mpileup.vcf.gz --out samq3filt --remove-indels --max-maf .2 --recode
vcftools --gzvcf allsamp.mpileup.vcf.gz --out samq2filt  --minQ 30 --max-maf .2 --recode

##########################################################################################
12/10/15
# Decided to call snps for all 119 samples for OP families
# In order to do this I need to merge the fastq files of 3 tech reps from all samples
cd /media/LGEP/Sequencing/filt.trim_rep1
ls *.fq.gz > allsamp.list --- this makes a  list of all files ending in .fq.gz in filt trim rep 1

#Now move allsamp.list to the Sequencing folder and run the the cat to merge the files in the list of allsamp.list:
cd ..    - This changes the directory from filt.trim_rep1 to the the sequencing directory
while read file; do cat filt.trim_rep1/$file filt.trim_rep2/$file filt.trim_rep3/$file > ./$file; done < .fq.gz.list
# These merged 3 tech reps are now in the /seagate/LGEP/Sequencing folder

for file in *.fq.gz; do bwa075 mem -t 12 -R "@RG\tID:${file}\tSM:${file}" Pita.86ktxptome.fa ${file} | samtools view -buF2308 -q20 - | samtools sort -l 5 -@ 5 -m 4G -T ./${file} -o ./bam/${file}.bam ; samtools index ./bam/${file}.bam; echo -e "@RG\tID:${file}\tSM:${file}" >> targets.header ; done 

#For some reason this only did 115 out of the 144. Going back and finishing the other 30~
- moved the 29 files to a new folder and then created a list
ls *.fq.gz >> samplist.list
- then moved those 29 files back to the Sequencing folder with the samplist.list object and ran the code below
while read file; do bwa075 mem -t 12 -R "@RG\tID:${file}\tSM:${file}" Pita.86ktxptome.fa ${file} | samtools view -buF2308 -q20 - | samtools sort -l 5 -@ 5 -m 4G -T ./${file} -o ./bam/${file}.bam ; samtools index ./bam/${file}.bam; echo -e "@RG\tID:${file}\tSM:${file}" >> targets.header ; done < samplist.list
-now went to bam folder where the bam files were and created a list of all bam files:
ls *.bam >> bamlist.list

-Went through this list and deleted families so that only the 119/144 OP families were present
-Then rean freebayes on the new saved list as shown below:
freebayes -L bamlist.list -v ./OPsamples.vcf -f ../Pita.86ktxptome.fa -K

vcftools --vcf OPsamples.vcf --out freeqfilt --remove-indels --max-maf .2 --recode

vcftools --vcf freeqfilt.recode.vcf --relatedness2

#This matrix was created and incorporated in the two 119 OP families, it did not improve results

 
##########################################################################################
#6/25 coming back to filtered and trimmed count files in /media/seagate/analysis/filttrim_rep*
	These files are the *.fq.gz files in the filttrim_rep1 rep2 and rep3 directories
	The files were filtered and trimmed all in one step using the bbduk command previosuly
#Now Aligning them to the transcriptome using bowtie2
#Ross created a transcirptome file v101.scaffolds.fa
#Then ran the "bowtie2-build" command which created 6 output files with *.bt2 file #extension. The base name of the index is v101.scaff 

arfesta@fairfax:/media/sg3/ptaeda/bowtie$ for file in /media/seagate/LGEP/filttrim_rep2/*.fq.gz; do /home/ross/software/bowtie2-2.2.1/bowtie2 -p 22 -x v101.scaff -U "$file" -S "${file%}rep2.sam" ; done

#The code above executes the following command:
for every file in the filttrim_rep2 directory
do the bowtie2 command(pointing to the directory of the program)
-p 22 : utilize 22 cpu cores for multithreading
-x v101.scaff: the input index file basename is v101.scaff
-U "$file" :  the specified fq.gz read count file to align is the input file
-S "" : returns sam format and specifying unique filename for each file in directory
#The code returns sam files into the filtrep directory where the files are being indexed #from.  At this time the total available harddrive space for seagate was 22%, so I moved #the files to the /media/sg3/ptaeda/bowtie/rep2samfiles folder.
#After file transfer was complete, I re-ran the loop above for rep3 and rep1 following #the same steps as above.

##Next step is to convert SAM files to BAM files
# http://biobits.org/samtools_primer.html  -- this website shows great tutorial for how #to use samtools to identify SNPs

for file in *.sam; do samtools view -b -S -o /media/seagate/LGEP/Sequencing/filt.trim_rep1/$file.bam $file  ; done

samtools view -bS 64p.fq.gzrep1.sam| samtools sort - 64rep1_sorted 

for file in *.sam; do samtools view -b -S -o /media/seagate/LGEP/Sequencing/filt.trim_rep1/$file.bam $file  ; done

#Converted SAM files to BAM files one by one and they are output in the rep2samfiles  ##direcotry
#The same loop above was ran for rep1samfiles and rep3 samfiles.

###The below was done for only 48 files (suppose to be 47 but 55q was replaced with 57e at a later point so 57e was done by itself in bamaddrg, samtools sort, and then was manually put in the freebayes.list file replacing 55q and then merged

#Now take 1 biological rep for every family and run bamaddrg which will add read groups #and merge technical reps for 1 sample from each family
##First created samples.list, in ptaeda/bowtie directory, which contains list of 1 sample name from each family (1o,4s,7d, ect..).

arfesta@fairfax:/media/sg3/ptaeda/bowtie$ while read lane; do /home/ross/software/bamaddrg/bamaddrg -b rep1samfiles/${lane}.fq.gzrep1.sam.bam -s $lane \ -b /media/seagate/LGEP/Sequencing/filt.trim_rep2/${lane}.fq.gzrep2.sam.bam -s $lane \ -b /media/seagate/LGEP/filt.trim_rep3/${lane}.fq.gzrep3.sam.bam -ts $lane > /media/seagate/LGEP/Sequencing/bamfiles/${lane}.bam; done < allsamp.list

/home/ross/software/bamaddrg/bamaddrg -b rep1samfiles/100n.fq.gzrep1.sam.bam -s 100n \ -b /media/seagate/LGEP/Sequencing/filt.trim_rep2/100n.fq.gzrep2.sam.bam -s 100n \ -b /media/seagate/LGEP/filt.trim_rep3/100n.fq.gzrep3.sam.bam -s 100n > /media/seagate/LGEP/Sequencing/bamfiles/100n.bam

#moved the output to bamaddrg folder in bowtie (7/6/15)
#The above code merges the bam files from rep1 rep2 and rep3samfiles folders matching the #sample name (1o,4s ect..)

##Now that we have 1 biological replicate bam file (which is a merger of 3 technical rep #bam files that have their corresponding read groups added) we can sort the merged bam #files by running the command below

for file in *.bam; do samtools sort -T align.sorted/$file -m 3G -@6 -o $file.sort.bam $file ; done

samtools sort -m 3G -@6 -o 85u.bam.sort.bam  85u.bam

#The above command uses samtools sort with a temporary output folder called align.sorted ##which is in the current working directory. -m 2G means to assign 2GB of space -@5 means ##to use 5 threads.  -o specifies the name of the out file. -O specifies the output will ##be a bam file. 144w.bam specifies the name of the input file
##**Note that the sample id needs to be changed each time in the 3 places for all samples 
#	In this case all samples refers to the sample ids in samples.list which are 1 from each of the 47 different families 

#In order to merge the 47 different bam files that are now aligned and sorted, first ###create a list of the samples that need to be merged by using the find option.
#Then use the samtools merge function to merge the 47 bam files
#Finding all of the files that end in .sort.bam and creating a list out of the filenames ####into the output list "freebayesamples.list"

find *.sort.bam  -printf "%f " > freebayesamples.list

#55q.sort.bam was a repeat of family 25-75 in the list of 47 --- it was suppose to be sample 57e
##57e was combined for the three reps, and then sorted using samtools

for file in *.sort.bam; do samtools merge -r -@18 -b
samtools merge -r -@16 -b freebayesamples.list 47.family.merged.bam

##Now we can use samtools index and create a index out of the merged bam file
samtools index 47.family.merged.bam
#this creates 47.family.merged.bam.bai file that is the index for the merged bam file
# ***The index file for bam (.bai) is a companion file that has the same name as the bam and help sto index the bam file for downstream uses

#Now move the .bai and 47.family.merged.bam file to the bowtie directory which is the #####same direcotyr that has the fasta reference.


#use freebayes to create SNP matrix VCF output  - 7/5, 7/10 (with new merged 47 family)

for file in *.sort.bam; do freebayes -v $file.vcf -f v101.scaffolds.fa -b $file; done
#The vcf output is called 45.vcf and is in the bowtie directory
#run on 47 families (3 technical reps for each family) took 31 hours
samtools mpileup -g -f v101.scaffolds.fa 104a.bam.sort.bam > 104a.bcf
bcftools call -c -v 104a.bcf > 104a.vcf


#use vcftools to filter vcf output from freebayes and then create new vcf file as output

arfesta@fairfax:/media/sg3/ptaeda/bowtie$ vcftools --vcf 104a.vcf --out freeqfilt  --minQ 30 --remove-indels --remove-filtered-all --hwe .001 --recode

#After filtering, kept 1,160,976 out of possible 3,522,791 sites
# Second time: After filtering, kept 1,162,872 out of a possible 3,527,522 Sites
#moved output file "freeqfilt.recode.vcf" to a new folder "free.vcf.filt" 
#Now use vcf-stats to collect stat info on new filtered vcf file

arfesta@fairfax:/media/sg3/ptaeda/bowtie/free.vcf.filt$ vcf-stats freeqfilt.recode.vcf -p freeqfilt.stats
#This outputs summary statistics such as snps per sample and private snps per sample

#Now use vcftools to create covariance relationship matrix using snps and print out ##private snps, het stat

vcftools --vcf freeqfilt.recode.vcf --relatedness2
 #out.relatedness2
vcftools --vcf freeqfilt.recode.vcf --relatedness
 #out.relatedness
vcftools --vcf freeqfilt.recode.vcf --het
 #out.het 
vcftools --vcf freeqfilt.recode.vcf --singletons
 #out.singletons

#######################################
##Also use samtools mpileup to see the difference - first create bcf uncompressed and then pipe that to use bcftools to call vcf format, multi-allelic, vcf file type, and then file name.

arfesta@fairfax:/media/sg3/ptaeda/bowtie$ for file in *.sort.bam; do samtools mpileup -ugf v101.scaffolds.fa  $file | bcftools call -vmO z -o ${file%.*}.vcf.gz ; done

#took around 24 hours

# STOPPED HERE ON REPEAT RUN FOR NEW SAMPLE 7/10

#now use vcftools to filter study.vcf.gz
vcftools --gzvcf samtools.47.vcf.gz --out samqfilt  --minQ 30 --remove-indels --remove-filtered-all --hwe .001 --recode

#Using zlib version: 1.2.3.4
#Versions of zlib >= 1.2.4 will be *much* faster when reading zipped VCF files.
#First time: After filtering, kept 1,499,611 out of a possible 3,300,706 Sites
#Second time 7/13: After filtering, kept 1,497,107 out of a possible 3,304,542 Sites
#output samqfilt.recode.vcf moved to folder sam.vcf.filt

#Now run vcf-stats on samqfilt vcf file
vcf-stats samqfilt.recode.vcf -p samqfilt.stats

#additional stats for samqfilt.recode.vcf
vcftools --vcf samqfilt.recode.vcf --relatedness2
 #out.relatedness2
vcftools --vcf samqfilt.recode.vcf --relatedness
 #out.relatedness
vcftools --vcf samqfilt.recode.vcf --het
 #out.het 
vcftools --vcf samqfilt.recode.vcf --singletons
 #out.singletons

######################################################################
######################################################################
##	Second half of missing lanes have arrived 7/9/15
##		
#Unpack tar archives filter and trim the sequences, and produce output ready to analyze. #Rather than merging with the half-lane-worth of each pool from the initial run, we might #try keeping these separate and have a more balanced dataset to see how that affects the #outcome of the clustering routine - we can merge later if we decide that is the right #approach.

#Each of these 6 lanes have been previously sequenced and are in their corresponding #lane0*_* folder in the LGEP directory.  We rename the above tar files to match the lane #and rep that the samples were in in the previous sequencing run
#ex. Y5_2.tar is the re-run of lane05_2 run. The samples in Y5_2.tar should be the same #as samples in lane05_2 run.  After unpacking tar, we rename this new completed lane to #be lane05_2_2 indicating that it is lane05 rep 2 and the 2nd run.

#Note: This time after unpacking the tar files (Y5_2.tar, Y6_2.tar, ..., Z4_2.tar), a #given sample in each lane (Y5_2 being lane 5 rep 2 and _2 meaning this is the second #sequencing batch) had 3 fastq.gz files instead of 1.

#We concatenate the three fastq.gz files in each sample
#THIS WORKS for single file
cat Sample_25e/*.fastq.gz > Sample_25e.fastq.gz

#THIS WORKS to do this on a given direcotry (i.e. lane05_2_2 = Y5_2.tar)
arfesta@fairfax:/media/seagate/LGEP/lane.tarfiles/lane04_3_2$ for f in Sample_*; do (cd $f && cat *.fastq.gz > ../${f}.fastq.gz); done

#Then change the directory to the next lane (i.e. lane06_2_2) and run again for all 6 new anes

#Now run fastqc on all new data that has not been filtered or trimmed
NEED TO DO

#Now run bbduk to filter and trim 
while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane06_2_2/Sample_${id}.fastq.gz out=../filttrim_rep2/${id}2.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane06_2_2/*.gz |  sed 's%lane06_2_2/Sample_%%' | sed 's/.fastq.gz//' )

#then run fastqc to see filtered data
cd filttrim_rep3
for file in "*2.fastq.gz"; do fastqc ${file}; done

#Now make list of the new samples in the new 6 lanes that sailfish needs to be run on
cd filttrim_rep2
ls *2.fq.gz | sed 's/.fq.gz//' >> rep2lanestorunsailfish.list
cd filttrim_rep3
ls *2.fq.gz | sed 's/.fq.gz//' >> rep3lanestorunsailfish.list

#move these new lists to the LGEP directory and then run the code below to execute sailfish

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do sailfish quant -i /media/sg3/ptaeda/Pita.86ktxptome.sfidx -l "T=SE:S=S" -r <(zcat filttrim_rep2/${id}.fq.gz) -o countfiles/${id}_rep2 -p 6 -a; done < rep2lanestorunsailfish.list

export LD_LIBRARY_PATH=/home/ross/software/sailfish/lib
while read id; do sailfish quant -i /media/sg3/ptaeda/Pita.86ktxptome.sfidx -l "T=SE:S=S" -r <(zcat filttrim_rep3/${id}.fq.gz) -o countfiles/${id}_rep3 -p 6 -a; done < rep3lanestorunsailfish.list

#Now the LGEP/countfiles folder contains the countfiles for the first 18 lanes of #sequencing plus the additional new 6 lanes of sequencing

#In order to use readincountfiles.R we need to remove the old 6 lanes from the countfiles #folder that the new 6 lanes are replacing

#First make one list that contains the file names for the old 6 lanes
OLD 6 LANES ARE: lane05_2, lane06_2, lane01_3, lane02_3, lane03_3, lane04_3 
cd LGEP/all.lane.data
ls lane05_2/*.gz |  sed 's%lane05_2/Sample_%%' | sed 's/.fastq.gz//' >> rep2countfilestoremove.list

ls lane06_2/*.gz |  sed 's%lane06_2/Sample_%%' | sed 's/.fastq.gz//' >> rep2countfilestoremove.list

arfesta@fairfax:/media/seagate/LGEP/countfiles$ while read id; do mv ${id}_rep2 ../oldcountfiles ; done < rep2countfilestoremove.list

####THIS WORKS but permission denied -- Ross will do.
cd LGEP/all.lane.data
ls lane01_3/*.gz |  sed 's%lane01_3/Sample_%%' | sed 's/.fastq.gz//' >> rep3countfilestoremove.list
ls lane02_3/*.gz |  sed 's%lane02_3/Sample_%%' | sed 's/.fastq.gz//' >> rep3countfilestoremove.list
ls lane03_3/*.gz |  sed 's%lane03_3/Sample_%%' | sed 's/.fastq.gz//' >> rep3countfilestoremove.list
ls lane04_3/*.gz |  sed 's%lane04_3/Sample_%%' | sed 's/.fastq.gz//' >> rep3countfilestoremove.list

arfesta@fairfax:/media/seagate/LGEP/countfiles$ while read id; do mv ${id}_rep3 ../oldcountfiles ; done < rep3countfilestoremove.list

###NOW WE CAN READ IN THE NEW DATA SET WITH UPDATED 6 LANES INTO R using readincountfiles.R in the LGEP/analysis folder
###################################################################################
###################################################################################
###################################################################################

#Attempting to retrive snps for 130f, 52w, and 104a that have .49-.6 frequency
# going to use bcftools but first need to index file with tabix
# to index file with tabix it must be vcf.gzf

bowtie/free.vcf.filt$ bgzip -c freeqfilt.recode.vcf > freeqfilt.recode.vcf.gzf

bowtie/free.vcf.filt$ tabix -p vcf freeqfilt.recode.vcf.gz


bcftools view -a -s 52w,104a,130f -q .4 -Q .6 -v snps freeqfilt.recode.vcf.gz > 11-1111.

#####################################################################################
#####################################################################################
#####################################################################################
#9/5/15 - - using sailfish to get new counts using a new annotation file from pinerefseq
#The new annotation file is pita.HQgenes.fasta and was uploaded to site 8/17/15
#First we need to build index from transcript counts for the new annotation file
#Note: couldn't update to new version of sailfish, but as consequence using sailfish.old directory
###need to be in this sailfish.old directory to execute sailfish
export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
#Build
sh index of pita.HQgenes.fasta -- new annotation file 8/17/15 from pinerefseq
./sailfish index -t /home/arfesta/sailfish.w.updated.annontation.8.17/pita.HQgenes.fasta -o /home/arfesta/sailfish.w.updated.annontation.8.17/pita.HQgenes.sfidx -k 25 -p 20

#index of pita.HQ.genes is now in sailfish.w.updated.annotation.8.17 folder in home directory with .fasta t counts
##
#Due to bringing in of new sequencing lanes which were suppose to fix the problem with previous lanes, files are disorganized
#In lane.tar directory, created old.lane.tar directory which contains the first set of 6 sequencing lanes
#I renamed the new set of 6 sequencing lanes Y5-Z4 as the other tar files, their date is june 7th different than the other files
#Also went into all.lane.data and put old lanes into a new subfolder, as well as renamed the new lanes from previous lane0*_*_2 to the
###format of others: example lane05_2_2 lane 5 of rep 2, 2nd sequencing effort renamed to lane05_2

#Now becuase filttrim_rep1,2,3 have other files in them from trimming new data set, creating 3 new directories to hold new filt trim of #counts
#Three new directories in all.lane.data include: filt.trim_rep1,rep2,rep3.

#Now executing bbduk within the all.lane.data folder and putting all trimmed and filtered counts in the filt.trim_rep* directory
arfesta@fairfax:/media/seagate/LGEP/Sequencing/all.lane.data$ while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane01_3/Sample_${id}.fastq.gz out=../filt.trim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane01_3/*.gz |  sed 's%lane01_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane02_3/Sample_${id}.fastq.gz out=../filt.trim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane02_3/*.gz |  sed 's%lane02_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane03_3/Sample_${id}.fastq.gz out=../filt.trim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane03_3/*.gz |  sed 's%lane03_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane04_3/Sample_${id}.fastq.gz out=../filt.trim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane04_3/*.gz |  sed 's%lane04_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane05_3/Sample_${id}.fastq.gz out=../filt.trim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane05_3/*.gz |  sed 's%lane05_3/Sample_%%' | sed 's/.fastq.gz//' )

while read id; do /home/ross/software/bbmap/bbduk.sh -Xmx2g in=lane06_3/Sample_${id}.fastq.gz out=../filt.trim_rep3/${id}.fq.gz ref=/home/ross/software/bbmap/resources/truseq.fa.gz forcetrimleft=10 t=1 ktrim=r k=25 mink=12 hdist=1 qtrim=rl trimq=20 minlength=50; done < <(ls lane06_3/*.gz |  sed 's%lane06_3/Sample_%%' | sed 's/.fastq.gz//' )

#The above code is for rep 1, we can change to rep2 or 3 by adding _2 or _3 to the "lane0*".  Also change output to the filt.trim rep it belongs to, so for rep2 it would be filt.trim_rep2 as the output location
###################################
###########################################
#####################################
#Now make list of the samples that sailfish needs to be run on
cd filt.trim_rep1
ls *.fq.gz | sed 's/.fq.gz//' >> rep1lanestorunsailfish.list
cd filt.trim_rep2
ls *.fq.gz | sed 's/.fq.gz//' >> rep2lanestorunsailfish.list
cd filt.trim_rep3
ls *.fq.gz | sed 's/.fq.gz//' >> rep3lanestorunsailfish.list

#move these new lists to the home directory and then run the code below to execute 
#sailfish. Mapping reads to newly annotated count file

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /home/arfesta/sailfish.w.updated.annontation.8.17/pita.HQgenes.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep1/${id}.fq.gz) -o /home/arfesta/sailfish.w.updated.annontation.8.17/countdata/${id}_rep1 -p 8 -a; done < rep1lanestorunsailfish.list

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /home/arfesta/sailfish.w.updated.annontation.8.17/pita.HQgenes.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep2/${id}.fq.gz) -o /home/arfesta/sailfish.w.updated.annontation.8.17/countdata/${id}_rep2 -p 8 -a; done < rep2lanestorunsailfish.list

while read id; do ./sailfish quant -i /home/arfesta/sailfish.w.updated.annontation.8.17/pita.HQgenes.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep3/${id}.fq.gz) -o /home/arfesta/sailfish.w.updated.annontation.8.17/countdata/${id}_rep3 -p 8 -a; done < rep3lanestorunsailfish.list

########################################################
###Did the same for these two fasta files(one is full 83000 genes, other is filtered 21000 genes), #creating index with sailfish 
##Moved the folder sailfish.w.v101.transcriptome to SEAGATE2 -- contains 2 transcriptome maps to align to
./sailfish index -t /home/arfesta/sailfish.w.v101.transcriptome/pita.transcriptome.cds.fasta -o /home/arfesta/sailfish.w.v101.transcriptome/pita.transcriptome.cds.sfidx -k 25 -p 20

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /media/seagate2/LGEP/sailfish.w.v101.transcriptome/pita.transcriptome.cds.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep1/${id}.fq.gz) -o /media/seagate2/LGEP/sailfish.w.v101.transcriptome/countdata.cds/${id}_rep1 -p 8 -a; done < rep1lanestorunsailfish.list

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /media/seagate2/LGEP/sailfish.w.v101.transcriptome/pita.transcriptome.cds.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep2/${id}.fq.gz) -o /media/seagate2/LGEP/sailfish.w.v101.transcriptome/countdata.cds/${id}_rep2 -p 8 -a; done < rep2lanestorunsailfish.list

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /media/seagate2/LGEP/sailfish.w.v101.transcriptome/pita.transcriptome.cds.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep3/${id}.fq.gz) -o /media/seagate2/LGEP/sailfish.w.v101.transcriptome/countdata.cds/${id}_rep3 -p 8 -a; done < rep3lanestorunsailfish.list

#Here is the filtered dataset --- I will not try this to begin with*
./sailfish index -t /home/arfesta/sailfish.w.v101.transcriptome/Pita.IU.TranscriptomeMainsV1.102013.fasta -o /home/arfesta/sailfish.w.v101.transcriptome/Pita.IU.TranscriptomeMainsV1.102013.sfidx -k 25 -p 20

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /media/seagate2/LGEP/sailfish.w.v101.transcriptome/Pita.MainsV1.1013.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep1/${id}.fq.gz) -o /media/seagate2/LGEP/sailfish.w.v101.transcriptome/countdata.full/${id}_rep1 -p 5 -a; done < rep1lanestorunsailfish.list

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /media/seagate2/LGEP/sailfish.w.v101.transcriptome/Pita.MainsV1.1013.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep2/${id}.fq.gz) -o /media/seagate2/LGEP/sailfish.w.v101.transcriptome/countdata.full/${id}_rep2 -p 5 -a; done < rep2lanestorunsailfish.list

export LD_LIBRARY_PATH=/home/ross/software/sailfish.old/lib
while read id; do ./sailfish quant -i /media/seagate2/LGEP/sailfish.w.v101.transcriptome/Pita.MainsV1.1013.sfidx -l "T=SE:S=S" -r <(zcat /media/seagate/LGEP/Sequencing/filt.trim_rep3/${id}.fq.gz) -o /media/seagate2/LGEP/sailfish.w.v101.transcriptome/countdata.full/${id}_rep3 -p 5 -a; done < rep3lanestorunsailfish.list


for file in *.sam; do samtools view -bS file.sam | samtools sort - file_sorted

